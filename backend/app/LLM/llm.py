import os
from typing import Optional, Literal, Iterator

from openai import OpenAI
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log
)
import logging

from app.cache import get_llm_cache
from app.config import get_settings

# æ”¯æŒçš„LLMæä¾›å•†
SUPPORTED_PROVIDERS = Literal[
    "openai",
    "deepseek",
    "qwen",
    "modelscope",
    "kimi",
    "zhipu",
    "ollama",
    "vllm",
    "local",
    "auto",
    "custom",
    "siliconflow"
]

class LpyAgentsLLM:
    """
      è‡ªå®šä¹‰çš„LLMå®¢æˆ·ç«¯ã€‚
      å®ƒç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚

      è®¾è®¡ç†å¿µï¼š
      - å‚æ•°ä¼˜å…ˆï¼Œç¯å¢ƒå˜é‡å…œåº•
      - æµå¼å“åº”ä¸ºé»˜è®¤ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
      - æ”¯æŒå¤šç§LLMæä¾›å•†
      - ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
      """

    def __init__(
            self,
            model: Optional[str] = None,
            api_key: Optional[str] = None,
            base_url: Optional[str] = None,
            provider: Optional[SUPPORTED_PROVIDERS] = None,
            temperature: float = 0.7,
            max_tokens: Optional[int] = None,
            timeout: Optional[int] = None,
            **kwargs
    ):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚
        æ”¯æŒè‡ªåŠ¨æ£€æµ‹provideræˆ–ä½¿ç”¨ç»Ÿä¸€çš„LLM_*ç¯å¢ƒå˜é‡é…ç½®ã€‚

        Args:
            model: æ¨¡å‹åç§°ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡LLM_MODEL_IDè¯»å–
            api_key: APIå¯†é’¥ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            base_url: æœåŠ¡åœ°å€ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡LLM_BASE_URLè¯»å–
            provider: LLMæä¾›å•†ï¼Œå¦‚æœæœªæä¾›åˆ™è‡ªåŠ¨æ£€æµ‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¶…æ—¶æ—¶é—´ï¼Œä»ç¯å¢ƒå˜é‡LLM_TIMEOUTè¯»å–ï¼Œé»˜è®¤60ç§’
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        self.model = model or os.getenv("LLM_MODEL_ID")
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout or int(os.getenv("LLM_TIMEOUT", "30"))
        self.kwargs = kwargs

        # è‡ªåŠ¨æ£€æµ‹provideræˆ–ä½¿ç”¨æŒ‡å®šçš„provider
        requested_provider = (provider or "").lower() if provider else None
        self.provider = provider or self._auto_detect_provider(api_key, base_url)

        if requested_provider == "custom":
            self.provider = "custom"
            self.api_key = api_key or os.getenv("LLM_API_KEY")
            self.base_url = base_url or os.getenv("LLM_BASE_URL")
        else:
            # æ ¹æ®providerç¡®å®šAPIå¯†é’¥å’Œbase_url
            self.api_key, self.base_url = self._resolve_credentials(api_key, base_url)

        # éªŒè¯å¿…è¦å‚æ•°
        if not self.model:
            self.model = self._get_default_model()
        if not all([self.api_key, self.base_url]):
            raise Exception("APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        self._client = self._create_client()

        # é…ç½®æ—¥å¿—
        self._logger = logging.getLogger(__name__)

        # è·å–é‡è¯•é…ç½®
        settings = get_settings()
        self._retry_max_attempts = settings.llm_retry_max_attempts
        self._retry_wait_min = settings.llm_retry_wait_min
        self._retry_wait_max = settings.llm_retry_wait_max
        self._retry_multiplier = settings.llm_retry_multiplier

    def _auto_detect_provider(self, api_key: Optional[str], base_url: Optional[str]) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹LLMæä¾›å•†

        æ£€æµ‹é€»è¾‘ï¼š
        1. ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
        2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        3. æ ¹æ®base_urlåˆ¤æ–­
        4. é»˜è®¤è¿”å›é€šç”¨é…ç½®
        """
        # 1. æ£€æŸ¥ç‰¹å®šæä¾›å•†çš„ç¯å¢ƒå˜é‡
        if os.getenv("OPENAI_API_KEY"):
            return "openai"
        if os.getenv("DEEPSEEK_API_KEY"):
            return "deepseek"
        if os.getenv("DASHSCOPE_API_KEY"):
            return "qwen"
        if os.getenv("MODELSCOPE_API_KEY"):
            return "modelscope"
        if os.getenv("KIMI_API_KEY") or os.getenv("MOONSHOT_API_KEY"):
            return "kimi"
        if os.getenv("ZHIPU_API_KEY") or os.getenv("GLM_API_KEY"):
            return "zhipu"
        if os.getenv("OLLAMA_API_KEY") or os.getenv("OLLAMA_HOST"):
            return "ollama"
        if os.getenv("VLLM_API_KEY") or os.getenv("VLLM_HOST"):
            return "vllm"
        if os.getenv("SILICONFLOW_API_KEY") or os.getenv("SILICON_CLOUD_API_KEY"):
            return "siliconflow"

        # 2. æ ¹æ®APIå¯†é’¥æ ¼å¼åˆ¤æ–­
        actual_api_key = api_key or os.getenv("LLM_API_KEY")
        if actual_api_key:
            actual_key_lower = actual_api_key.lower()
            if actual_api_key.startswith("ms-"):
                return "modelscope"
            elif actual_key_lower == "ollama":
                return "ollama"
            elif actual_key_lower == "vllm":
                return "vllm"
            elif actual_key_lower == "local":
                return "local"
            elif actual_api_key.startswith("sk-") and len(actual_api_key) > 50:
                # å¯èƒ½æ˜¯OpenAIã€DeepSeekæˆ–Kimiï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ¤æ–­
                pass
            elif actual_api_key.endswith(".") or "." in actual_api_key[-20:]:
                # æ™ºè°±AIçš„APIå¯†é’¥æ ¼å¼é€šå¸¸åŒ…å«ç‚¹å·
                return "zhipu"

        # 3. æ ¹æ®base_urlåˆ¤æ–­
        actual_base_url = base_url or os.getenv("LLM_BASE_URL")
        if actual_base_url:
            base_url_lower = actual_base_url.lower()
            if "api.openai.com" in base_url_lower:
                return "openai"
            elif "api.deepseek.com" in base_url_lower:
                return "deepseek"
            elif "dashscope.aliyuncs.com" in base_url_lower:
                return "qwen"
            elif "api-inference.modelscope.cn" in base_url_lower:
                return "modelscope"
            elif "api.moonshot.cn" in base_url_lower:
                return "kimi"
            elif "open.bigmodel.cn" in base_url_lower:
                return "zhipu"
            elif "api.siliconflow.cn" in base_url_lower:
                return "siliconflow"
            elif "localhost" in base_url_lower or "127.0.0.1" in base_url_lower:
                # æœ¬åœ°éƒ¨ç½²æ£€æµ‹ - ä¼˜å…ˆæ£€æŸ¥ç‰¹å®šæœåŠ¡
                if ":11434" in base_url_lower or "ollama" in base_url_lower:
                    return "ollama"
                elif ":8000" in base_url_lower and "vllm" in base_url_lower:
                    return "vllm"
                elif ":8080" in base_url_lower or ":7860" in base_url_lower:
                    return "local"
                else:
                    # æ ¹æ®APIå¯†é’¥è¿›ä¸€æ­¥åˆ¤æ–­
                    if actual_api_key and actual_api_key.lower() == "ollama":
                        return "ollama"
                    elif actual_api_key and actual_api_key.lower() == "vllm":
                        return "vllm"
                    else:
                        return "local"
            elif any(port in base_url_lower for port in [":8080", ":7860", ":5000"]):
                # å¸¸è§çš„æœ¬åœ°éƒ¨ç½²ç«¯å£
                return "local"

        # 4. é»˜è®¤è¿”å›autoï¼Œä½¿ç”¨é€šç”¨é…ç½®
        return "auto"

    def _resolve_credentials(self, api_key: Optional[str], base_url: Optional[str]) -> tuple[str, str]:
        """æ ¹æ®providerè§£æAPIå¯†é’¥å’Œbase_url"""
        if self.provider == "openai":
            resolved_api_key = api_key or os.getenv("OPENAI_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = (base_url or os.getenv("LLM_BASE_URL")
                                 or "https://api.openai.com/v1")
            return resolved_api_key, resolved_base_url

        elif self.provider == "deepseek":
            resolved_api_key = api_key or os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = (base_url or os.getenv("LLM_BASE_URL")
                                 or "https://api.deepseek.com")
            return resolved_api_key, resolved_base_url

        elif self.provider == "qwen":
            resolved_api_key = api_key or os.getenv("DASHSCOPE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv(
                "LLM_BASE_URL") or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "modelscope":
            resolved_api_key = api_key or os.getenv("MODELSCOPE_API_KEY") or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api-inference.modelscope.cn/v1/"
            return resolved_api_key, resolved_base_url

        elif self.provider == "kimi":
            resolved_api_key = api_key or os.getenv("KIMI_API_KEY") or os.getenv("MOONSHOT_API_KEY") or os.getenv(
                "LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://api.moonshot.cn/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "zhipu":
            resolved_api_key = api_key or os.getenv("ZHIPU_API_KEY") or os.getenv("GLM_API_KEY") or os.getenv(
                "LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "https://open.bigmodel.cn/api/paas/v4"
            return resolved_api_key, resolved_base_url

        elif self.provider == "ollama":
            resolved_api_key = api_key or os.getenv("OLLAMA_API_KEY") or os.getenv("LLM_API_KEY") or "ollama"
            resolved_base_url = base_url or os.getenv("OLLAMA_HOST") or os.getenv(
                "LLM_BASE_URL") or "http://localhost:11434/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "vllm":
            resolved_api_key = api_key or os.getenv("VLLM_API_KEY") or os.getenv("LLM_API_KEY") or "vllm"
            resolved_base_url = base_url or os.getenv("VLLM_HOST") or os.getenv(
                "LLM_BASE_URL") or "http://localhost:8000/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "local":
            resolved_api_key = api_key or os.getenv("LLM_API_KEY") or "local"
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL") or "http://localhost:8000/v1"
            return resolved_api_key, resolved_base_url

        elif self.provider == "custom":
            resolved_api_key = api_key or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL")
            return resolved_api_key, resolved_base_url
        elif self.provider == "siliconflow":
            resolved_api_key = api_key or os.getenv("LLM_API_KEY")
            resolved_base_url = (base_url or os.getenv("LLM_BASE_URL")
                                 or "https://api.siliconflow.cn/v1")
            return resolved_api_key, resolved_base_url

        else:
            # autoæˆ–å…¶ä»–æƒ…å†µï¼šä½¿ç”¨é€šç”¨é…ç½®ï¼Œæ”¯æŒä»»ä½•OpenAIå…¼å®¹çš„æœåŠ¡
            resolved_api_key = api_key or os.getenv("LLM_API_KEY")
            resolved_base_url = base_url or os.getenv("LLM_BASE_URL")
            return resolved_api_key, resolved_base_url

    def _create_client(self) -> OpenAI:
        """åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
        return OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
            timeout=self.timeout
        )

    def _get_default_model(self) -> str:
        """è·å–é»˜è®¤æ¨¡å‹"""
        if self.provider == "openai":
            return "gpt-3.5-turbo"
        elif self.provider == "deepseek":
            return "deepseek-chat"
        elif self.provider == "qwen":
            return "qwen-plus"
        elif self.provider == "modelscope":
            return "Qwen/Qwen2.5-72B-Instruct"
        elif self.provider == "kimi":
            return "moonshot-v1-8k"
        elif self.provider == "zhipu":
            return "glm-4"
        elif self.provider == "ollama":
            return "llama3.2"  # Ollamaå¸¸ç”¨æ¨¡å‹
        elif self.provider == "vllm":
            return "meta-llama/Llama-2-7b-chat-hf"  # vLLMå¸¸ç”¨æ¨¡å‹
        elif self.provider == "local":
            return "local-model"  # æœ¬åœ°æ¨¡å‹å ä½ç¬¦
        elif self.provider == "custom":
            return self.model or "gpt-3.5-turbo"
        elif self.provider == "siliconflow":
            return self.model or "Qwen/Qwen2.5-72B-Instruct"
        else:
            # autoæˆ–å…¶ä»–æƒ…å†µï¼šæ ¹æ®base_urlæ™ºèƒ½æ¨æ–­é»˜è®¤æ¨¡å‹
            base_url = os.getenv("LLM_BASE_URL", "")
            base_url_lower = base_url.lower()
            if "modelscope" in base_url_lower:
                return "Qwen/Qwen2.5-72B-Instruct"
            elif "deepseek" in base_url_lower:
                return "deepseek-chat"
            elif "dashscope" in base_url_lower:
                return "qwen-plus"
            elif "moonshot" in base_url_lower:
                return "moonshot-v1-8k"
            elif "bigmodel" in base_url_lower:
                return "glm-4"
            elif "ollama" in base_url_lower or ":11434" in base_url_lower:
                return "llama3.2"
            elif ":8000" in base_url_lower or "vllm" in base_url_lower:
                return "meta-llama/Llama-2-7b-chat-hf"
            elif "localhost" in base_url_lower or "127.0.0.1" in base_url_lower:
                return "local-model"
            else:
                return "gpt-3.5-turbo"

    def think(self, messages: list[dict[str, str]],
              temperature: Optional[float] = None) -> Iterator[str]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›æµå¼å“åº”ã€‚
        è¿™æ˜¯ä¸»è¦çš„è°ƒç”¨æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æµå¼å“åº”ä»¥è·å¾—æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œå¦‚æœæœªæä¾›åˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„å€¼

        Yields:
            str: æµå¼å“åº”çš„æ–‡æœ¬ç‰‡æ®µ
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        # ä½¿ç”¨é‡è¯•æœºåˆ¶è°ƒç”¨ LLM
        response = self._think_with_retry(messages, temperature)

        # å¤„ç†æµå¼å“åº”
        print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
        for chunk in response:
            content = chunk.choices[0].delta.content or ""
            if content:
                print(content, end="", flush=True)
                yield content
        print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type(Exception),
        before_sleep=before_sleep_log(logging.getLogger(__name__), logging.WARNING),
        reraise=True
    )
    def _think_with_retry(self, messages: list[dict[str, str]], temperature: Optional[float] = None):
        """å¸¦é‡è¯•æœºåˆ¶çš„æµå¼ LLM è°ƒç”¨"""
        return self._client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature if temperature is not None else self.temperature,
            max_tokens=self.max_tokens,
            stream=True,
        )

    def invoke(self, messages: list[dict[str, str]], **kwargs) -> str:
        """
        éæµå¼è°ƒç”¨LLMï¼Œè¿”å›å®Œæ•´å“åº”ã€‚
        é€‚ç”¨äºä¸éœ€è¦æµå¼è¾“å‡ºçš„åœºæ™¯ã€‚
        æ”¯æŒç¼“å­˜ä»¥å‡å°‘APIè°ƒç”¨æˆæœ¬ã€‚
        """
        # å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä½œä¸ºç¼“å­˜é”®
        messages_str = str(messages)
        temperature = kwargs.get('temperature', self.temperature)
        max_tokens = kwargs.get('max_tokens', self.max_tokens)

        # å°è¯•ä»ç¼“å­˜è·å–
        llm_cache = get_llm_cache()
        cached_response = llm_cache.get(messages_str, self.model, temperature, max_tokens)
        if cached_response is not None:
            llm_cache.record_hit()
            print("âœ… å‘½ä¸­ç¼“å­˜æˆåŠŸ,ä»ç¼“å­˜è·å– LLM å“åº”")
            return cached_response.get('response', '')

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨ LLM
        llm_cache.record_miss()
        print("âŒ ç¼“å­˜æœªå‘½ä¸­,å¼€å§‹è°ƒç”¨ LLM æ¨¡å‹...")
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶è°ƒç”¨ LLM
        result = self._invoke_with_retry(messages, temperature, max_tokens, **kwargs)

        # å°†ç»“æœå­˜å…¥ç¼“å­˜
        if result:
            llm_cache.set(messages_str, result, self.model, temperature, max_tokens)

        return result

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type(Exception),
        before_sleep=before_sleep_log(logging.getLogger(__name__), logging.WARNING),
        reraise=True
    )
    def _invoke_with_retry(self, messages: list[dict[str, str]], temperature: float, max_tokens: Optional[int], **kwargs) -> str:
        """å¸¦é‡è¯•æœºåˆ¶çš„ LLM è°ƒç”¨"""
        response = self._client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **{k: v for k, v in kwargs.items() if k not in ['temperature', 'max_tokens']}
        )
        return response.choices[0].message.content

    def stream_invoke(self, messages: list[dict[str, str]], **kwargs) -> Iterator[str]:
        """
        æµå¼è°ƒç”¨LLMçš„åˆ«åæ–¹æ³•ï¼Œä¸thinkæ–¹æ³•åŠŸèƒ½ç›¸åŒã€‚
        ä¿æŒå‘åå…¼å®¹æ€§ã€‚
        """
        temperature = kwargs.get('temperature')
        yield from self.think(messages, temperature)
